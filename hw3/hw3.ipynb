{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Name: Hugh Krogh-Freeman\n",
    "\n",
    "Uni: hk2903\n",
    "\n",
    "Email: hk2903@columbia.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\", encoding=\"utf8\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\", encoding=\"utf8\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2523\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: \n",
    "SEED = 2903\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "I'm watching this on the Sci-Fi channel right now. It's so horrible I can't stop watching it! I'm a Videographer and this movie makes me sad. I feel b\n",
      "0\n",
      "Hello, this little film is interesting especially for an artist, film-maker or music creator or a visual artist, for:<br /><br />One can feel and exam\n",
      "1\n",
      "(NOTE: I thought I'd be the only one writing what I did below, but I see the others here agree. I guess it was pretty obvious - this was overdoing the\n",
      "0\n",
      "This was the first televised episode of the Columbo series (although it was filmed after \"Death Lends a Hand\")and it heralded one of the most successf\n",
      "1\n",
      "Hawked as THE MOST OFFENSIVE MOVIE EVER, GUARANTEED TO OFFEND EVERYONE- Guess what? It worked, I'm offended that we shelled out money to rent this. Tw\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 5103   11  120   19]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[ 7769  4941     3  2970     5    87     8     1   952   100    28  8848\n",
      " 11649     3  1576   654     5 10285   325    10]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 0.3918 - acc: 0.8100 - val_loss: 0.3263 - val_acc: 0.8654\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 142s 7ms/step - loss: 0.0404 - acc: 0.9873 - val_loss: 0.4894 - val_acc: 0.8358\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 142s 7ms/step - loss: 0.0033 - acc: 0.9992 - val_loss: 0.5895 - val_acc: 0.8546\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 150s 8ms/step - loss: 1.5245e-04 - acc: 1.0000 - val_loss: 0.6331 - val_acc: 0.8562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed90ded940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 142s 7ms/step - loss: 0.3996 - acc: 0.8008 - val_loss: 0.3137 - val_acc: 0.8692\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 154s 8ms/step - loss: 0.0451 - acc: 0.9850 - val_loss: 0.4698 - val_acc: 0.8460\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 143s 7ms/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.7383 - val_acc: 0.8392\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 146s 7ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.6949 - val_acc: 0.8436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed88b2a8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, 128, input_length=MAX_SEQUENCE_LENGTH)) \n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 484s 24ms/step - loss: 0.4314 - acc: 0.7953 - val_loss: 0.3822 - val_acc: 0.8466\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 483s 24ms/step - loss: 0.2022 - acc: 0.9236 - val_loss: 0.3415 - val_acc: 0.8630\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 477s 24ms/step - loss: 0.1029 - acc: 0.9633 - val_loss: 0.4301 - val_acc: 0.8412\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 483s 24ms/step - loss: 0.0628 - acc: 0.9788 - val_loss: 0.5566 - val_acc: 0.8586\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 483s 24ms/step - loss: 0.0599 - acc: 0.9785 - val_loss: 0.5892 - val_acc: 0.7818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed858f32e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 35772\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 38,088,002\n",
      "Trainable params: 812,102\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=False, \n",
    "    weights=[g_word_embedding_matrix])) \n",
    "model.add(LSTM(EMBEDDING_DIM, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 1649s 82ms/step - loss: 0.4858 - acc: 0.7642 - val_loss: 0.3773 - val_acc: 0.8300\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 1657s 83ms/step - loss: 0.2895 - acc: 0.8787 - val_loss: 0.2841 - val_acc: 0.8822\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 1655s 83ms/step - loss: 0.2448 - acc: 0.9008 - val_loss: 0.2758 - val_acc: 0.8898\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 1663s 83ms/step - loss: 0.2035 - acc: 0.9206 - val_loss: 0.2791 - val_acc: 0.8900\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 1653s 83ms/step - loss: 0.1616 - acc: 0.9370 - val_loss: 0.3137 - val_acc: 0.8882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed43098ef0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 248, 300)          270300    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 248, 300)          0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 248, 300)          90300     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 246, 150)          135150    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 246, 150)          0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 246, 300)          45300     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 244, 75)           67575     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 244, 75)           0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 244, 300)          22800     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 73200)             0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 600)               43920600  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 82,008,827\n",
      "Trainable params: 44,732,927\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=False, \n",
    "                    weights=[g_word_embedding_matrix])) \n",
    "model.add(Convolution1D(filters=EMBEDDING_DIM, kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "\n",
    "model.add(Convolution1D(filters=int(EMBEDDING_DIM/2), kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "\n",
    "model.add(Convolution1D(filters=int(EMBEDDING_DIM/4), kernel_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2*EMBEDDING_DIM))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 1908s 95ms/step - loss: 8.0658 - acc: 0.4989 - val_loss: 7.9849 - val_acc: 0.5046\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 1900s 95ms/step - loss: 8.0776 - acc: 0.4989 - val_loss: 7.9849 - val_acc: 0.5046\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 1901s 95ms/step - loss: 8.0776 - acc: 0.4989 - val_loss: 7.9849 - val_acc: 0.5046\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 1885s 94ms/step - loss: 8.0776 - acc: 0.4989 - val_loss: 7.9849 - val_acc: 0.5046\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 1908s 95ms/step - loss: 8.0776 - acc: 0.4989 - val_loss: 7.9849 - val_acc: 0.5046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec195c4550>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 38,088,002\n",
      "Trainable params: 812,102\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "model.add(Embedding(nb_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=False, \n",
    "    weights=[g_word_embedding_matrix])) \n",
    "model.add(LSTM(EMBEDDING_DIM, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compille it here according to instructions\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/7\n",
      "20000/20000 [==============================] - 1672s 84ms/step - loss: 0.4880 - acc: 0.7524 - val_loss: 0.3182 - val_acc: 0.8646\n",
      "Epoch 2/7\n",
      "20000/20000 [==============================] - 1662s 83ms/step - loss: 0.2903 - acc: 0.8796 - val_loss: 0.2973 - val_acc: 0.8792\n",
      "Epoch 3/7\n",
      "20000/20000 [==============================] - 1651s 83ms/step - loss: 0.2475 - acc: 0.9004 - val_loss: 0.2938 - val_acc: 0.8868\n",
      "Epoch 4/7\n",
      "20000/20000 [==============================] - 1653s 83ms/step - loss: 0.2103 - acc: 0.9169 - val_loss: 0.2839 - val_acc: 0.8826\n",
      "Epoch 5/7\n",
      "20000/20000 [==============================] - 1654s 83ms/step - loss: 0.1764 - acc: 0.9329 - val_loss: 0.3149 - val_acc: 0.8768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ec4aea438>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./weights/\"\n",
    "model_name = 'model_best'\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=2)\n",
    "bst_model_path = wt_dir + model_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "         callbacks=[early_stopping, model_checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 639s 26ms/step\n",
      "Accuracy: 89.19%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* In a dense layer every node is connected to every other node in the next layer [1]. A convolutional layer applies a convolution operation to the input [2]. LSTM stands for \"Long Short Term Memory\"; an LSTM layer is part of a recurrent neural network and is able to learn \"long-term dependencies\" [3].\n",
    "* Relu is the rectifier function, `g(x) = max(0, x)`, and serves as the activation function in a neural network (Goldberg, 44). Neural networks are prone to overfitting, which necessitates a form of regularization called \"dropout training.\" Dropout training prevents the network from relying on specific weights and works by randomly setting half the neurons to zero in the network for each training example (Goldberg, 47).\n",
    "* In Part A, the embedding layer always comes first because it provides the features, before which nothing can happen. The next step is the flatten the 2D layer into something 1D [5]. This is necessary to prepare the layer for the next dense layer which is fully connected and where the insights happen. Every layer needs an activation function; hence, the rectilinear units. Then the dense layer with dimension = 2 functions to map the input to a binary output, which is transformed by the softmax activation function. Part A took about 10 minutes to train. The big addition in Part B is an additional fully connected, dense layer, which took the same amount of time to train. Part C involves a recurrent network, which naturally took 40 minutes to train. This takes longer because the network has loops. Part D adds pretrained word embeddings and dropout training to the previous network, making it the most accurate so far, an average of 87.6%. But it took 2 hours and 18 minutes. I chose this architecture to use in part F due to the high accuracy. Part E includes three 1-dimensional convolutional layers instead of the LTSM layer in part D. This network took 2.5 hours to train, but achieved only about 50% accuracy, a clue that perhaps I implemented it incorrectly.\n",
    "* I understand how basic neural networks work, and I can construct them in Python. I learned that neural networks, although they require parameter tuning, can learn features thereby eliminating the need for feature engineering. And I learned to install Cuda, Keras, and the accompanying stack onto my laptop, from which I did some of the training.\n",
    "\n",
    "[1] `https://www.quora.com/Is-there-a-difference-between-hidden-layer-and-dense-layer-in-neural-networks` <br>\n",
    "[2] `https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional` <br>\n",
    "[3] `http://colah.github.io/posts/2015-08-Understanding-LSTMs/` <br>\n",
    "[4] Yoav Goldberg and Graeme Hirst. 2017. Neural Network Methods in Natural Language Processing. Morgan & Claypool Publishers. <br>\n",
    "[5] `https://keras.io/layers/embeddings/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
